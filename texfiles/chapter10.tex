\chapter{Machine Learning Algorithms}
	<TODO Chapter Machine Learning Algorithms : PROOF READ>
	
This chapter is an overview of common machine learning algorithms. First the algorithm itself is presented but perhaps more importantly the scenarios in which to use the algorithm is illustrated with an example. In this way you will add tools to your machine learning tool box but also know when to use them. 

\section{K-Nearest Neighbors Algorithm}
	<TODO Section K-Nearest Neighbors Algorithms : PROOF READ>
	
The \ac{KNN} algorithm is one of the simplest algorithms that is used in machine learning. The \ac{KNN} is used to group similar things together. The most popular application is the recommended products feature on some websites. Where you have selected to view an item, the website will take that item you viewed as an input to the \ac{KNN} algorithm, find the five or so items that are similar and present them to the user.

The \ac{KNN} algorithm uses a metric to determine how \emph{similar} another item is to it. The usefulness of the product recommendation engine is solely based on the distance metric used for the products. This can be a very complex decision since the relationship between items can be different for different people. 

The use of data mining or data gathering from users is a key to operating a good product recommendation algorithm. If you collect data from the users that if they select a certain item then they end up buying another this link can be established in your \ac{KNN} algorithm for a few different reasons. Capturing all the reasons will improve the results and usability.

To understand how the \ac{KNN} algorithm works we will first define our database of products. The database is denoted, $\mathbf{D}$ where each row describes a product. For each product there are a number of columns that list the attributes of the item. The number of attributes is up to the developer that can help in differentiating the products. For a particular product we can query the database to get all the attributes associated with the product, $\mathbf{D}_p = \mathbf{D}[p,:]$ where $1 \leq v \leq N_P$ and $N_P$ is the total number of products available.  

Next a user can request information about a particular product which is in say row $u$. After we know the user is interested in $\mathbf{D}_u$ then we can start our search for similar products. To search for similar products we first need to calculate the \emph{distance} between $\mathbf{D}_u$ and $\mathbf{D}_v$ where $1 \leq v \leq N_P$. 

We calculate the distance between the $\mathbf{D}_u$ and $\mathbf{D}_v$ which we denote, $d_{u,v} = \norm{\mathbf{D}_u-\mathbf{D}_v}$. Next we find the lowest $k$ distance measures $d_{u,v}$ and return the products to the user. 

A quick note on the algorithm's complexity. If we have a large number of products, $N_P$, we will need to search over the entire catalog of products. Of course we want to return the recommended products as quickly as possible to the user to provide a great user experience. We are able to pre-calculate the distances between the products and store the results. We can reduce the amount of calculation required while we are on the user's clock. 
	
\section{Linear Regression}
	<TODO Section Linear Regression : PROOF READ>
	
\index{Linear Regression}, in it's simplest form, models the relationship between an independent variable and a dependent variable. Once the relationship is characterized then any future scenario can be analyzed. The dependent variable can be predicted based on the independent variable for the new scenario. 

You may have already performed some Linear Regression and not realized it. If in high school or undergraduate studies you may have had a paper to write or a book to read. You can time yourself to determine how long it takes you to write or read one page. Then since you know how many total pages you have to read or write you can determine how long it will take you total to finish writing or reading your paper or book respectively. 

In this scenario the independent variable is the number of pages you need to write or read. The amount of time you spend writing or reading a page is the \emph{slope} or rate at which you accomplish the work. Then the simple multiplication of the number of pages times the rate results in the total time needed to finish the assignment. 

To formally map this to a mathematical model we first consider the linear equation we learned in school,

\begin{equation}
y = mx+b.
\label{eq:svlinmodel}
\end{equation}
\noindent
The independent variable is $x$ the total number of pages, $m$ is the rate or the amount of time per page, $y$ is the dependent variable or the total time to accomplish the task. We haven't discussed $b$ which could model a constant setup time to start writing or reading. In this scenario $b=0$ is probably a good assumption. 

Lets look at the book reading example a little closer. Lets say we are reading a fictional novel and we determine $m$. We can figure out how long it will take the read the book. Then we decide to read another fictional novel where the pages are about the same size. There is not really a reason to determine what $m$ is again we can just use the previous value of $m$. 

However, can we use the same $m$ for reading and absorbing the material from a dense mathematics book. No, in this case or $m$ value would be smaller. We would need to take more time on each page since not only are the pages longer but also we probably can't just read a page once and move on.

So a word of caution here is that if we \emph{train} under some circumstance to obtain a value for $m$. In this example, the \emph{training} was timing ourselves reading a page, or better yet reading 10 pages and averaging the results. If we \emph{train} under a particular circumstance $m$ should only be used for similar scenarios. 

The training under similar scenarios sounds obvious but there may be some applications where maybe there are underlying assumptions that we don't know we are making. In the next example we will see how this could be true in the housing market. 

Before we dive into the next example we should define the single variant model, like we saw above and the multi variant model, which we will see in the next example. In the signal variant case we only considered an independent variable of one dimension, total pages. Next we will look at a model that considers multiple variables that all need to be considered in the dependent variable. 

Our next example discusses a model for valuing a house. There are of course many factors that go into the value of a house. The size of the house in square feet is a metric that would influence the house value. How about number of bedrooms, bathrooms, size of kitchen, size of yard, school district quality, and age of the house. 

We just listed seven metrics that could be used to value a house. I'm sure you can double that list if you wanted but the point here is that we need to consider as many variables as we can to accurately predict the house's value. We know need to define an equation to model our multi variant system. We can change \eq{svlinmodel} to,

\begin{equation}
y = \mathbf{m}^T\mathbf{x} + b.
\end{equation}
\noindent
Here we have a vector $\mathbf{x}$ of numbers that each represent an aspect of the house. In our case $\mathbf{x}$ is a list of seven numbers. The first number, $\mathbf{x}_1$ is the square-footage of a particular house all the way up to the last number, $\mathbf{x}_7$ that represents the age of the house in months. 

Next we can define the vector $\mathbf{m}$ which is another list of numbers of the same length of $\mathbf{x}$ but $\mathbf{m}$ is a weighting or dollar amount associated with each metric of the house. The units assigned to $\mathbf{m}$ will influence how we use our model.

There are a few ways in which we could use this model. First we could come up with all the possible metrics of a house and organize them in $\mathbf{x}$ and $\mathbf{m}$ then do some market research in different neighborhoods in America and look at the going rates and try to calculate the $\mathbf{m}$ values for different places. Clearly New York City will be different than rural Iowa. If you are doing research on trends in housing maybe this is useful. 

We could also use this model in a different way and this way is think would be more applicable to a broader audience. In this model we populate $\mathbf{x}$ and $\mathbf{m}$ we metrics that are valuable to you. If you have to have a house that has more than five bedrooms you put the number of bedrooms on the list. If you want to have a lot of land then that is another metric to put into $\mathbf{x}$. After compiling the list you can then look at some houses that may meet some of your wishes the idea would be to use metrics that include both \emph{have to haves} and also \emph{like to haves} so that a house either doesn't meet the basic needs, exceeds basic needs. Then you can choose the house that exceeds the basic needs for the right price. 

The second use for this model then doesn't have to have an $\mathbf{m}$ vector that has dollar amounts tied to bathrooms, which is difficult to calculate. Instead we can have relative weights or percentages be used. If a smaller house is desired, like downsizing in retirement then the weight assigned to square-footage is higher when square-footage is low. In this case at the end of the calculation you can compare the weighted result for all the houses to determine which one is most appropriate for you and your family. 

In general \index{Linear Regression} models a relationship between independent variables and dependent variables. The relationship is characterized by the single scalar $m$, vector $\mathbf{m}$, or $\mathbf{m}$ can be defined as a matrix which relates the independent variables to multiple dependent variable with a different linear combination or weighting. No matter the number of variables the calculation of $\mathbf{m}$ should only be applied in the context for which it was calculated. For the housing example above $\mathbf{m}$ may need to be recalculated if the housing market changes or your budget changes. 


\section{Linear Discriminant Analysis}
	<TODO Section Linear Discriminant Analysis : PROOF READ>

\index{Linear Discriminant Analysis} is used to reduce the number of dimensions of a problem to enable timely execution of the calculations. Similar to \ac{PCA} the analysis is performed to first reduce the dimensions needs then to perform the task first set out to do. 

In this section we will look at the algorithm to do face recognition. To do facial recognition we first must train the algorithm to know who we can recognize. Assume we have an image of a person where $\mathbf{P}$ is $(M\times N)$ pixels. Furthermore we have a $\mathbf{P}$ for each person say a total of $N_T$ people we would like to be able to recognize. 

To make this example more concrete say we are implementing a security system where a camera is used at the front door. If any of the $N_T$ employees we have walk up to the door we can recognize them and allow them to come in, otherwise we can take further action as necessary. When an employee is hired we need to take a picture of them for a badge \ac{ID} which is common now. This same picture can be our $\mathbf{P}$ used for the new employee. 

Next we need to construct a vectorized representation of all the $N_T$ images we have. The new matrix, $\mathbf{T}$ is $(MN\times N_T)$ where the $\operatorname{vec}$ takes the columns of the matrix and appends them each under the first column. So the result of $\operatorname{vec}{\mathbf{P}}$ is a column vector of the $MN$ pixels. We do this for each image and construct $\mathbf{T}$,
	
\begin{equation}
\mathbf{T} = \left[\operatorname{vec}(\mathbf{P}_1),\dots,\operatorname{vec}(\mathbf{P}_{N_T})\right].
\end{equation}	
	
After the construction of $\mathbf{T}$ we can compute the \ac{SVD} of $\mathbf{T}$. The \ac{SVD} is a computationally intensive algorithm but we will only need to do this in the training phase of the algorithm. We decompose $\mathbf{T}$ with \ac{SVD} as,

\begin{equation}
\mathbf{T}=\mathbf{U}\mathbf{\Sigma}\mathbf{V}^T.
\end{equation}

Now we can use the \ac{SVD} to get the eigenvalues and eigenvectors of the matrix $\mathbf{T}\mathbf{T}^T$. The matrix $\mathbf{U}$ already contains the eigenvectors. To get the eigenvalues we need to multiply the \ac{SVD} result for $\mathbf{T}$ with $\mathbf{T}^T$ like so,

\begin{eqnarray}
\mathbf{T}\mathbf{T}^T&=&\mathbf{U}\mathbf{\Sigma}\mathbf{V}^T\mathbf{V}\mathbf{\Sigma}^T\mathbf{U}^T\\
&=&\mathbf{U}\mathbf{\Sigma}\mathbf{\Sigma}^T\mathbf{U}^T\\
&=&\mathbf{U}\mathbf{\Lambda}\mathbf{U}^T.
\end{eqnarray}	

Now that we have the eigenvalues we can reduce dimensions, via the \ac{PCA} method but removing the dimensions with the smaller eigenvalues. We will now define $\mathbf{U}_{PCA}$ that has $k$ eigenvectors that correspond to the $k$ largest eigenvalues. 

For the \ac{LDA} we need to define two matrices. The first we already calculated which is the \emph{within} image scatter. We will denote the \emph{within} covariance as $\mathbf{S}_w$ which is going to be our $\mathbf{T}\mathbf{T}^T$. The second matrix we need to define is the \emph{between} image scatter, $\mathbf{S}_b$. To calculate $\mathbf{S}_b$ we first need the average across images, $\mathbf{\mu}_P = \expec{T}$. The result is a vector of length $NM$. This is opposed to the mean of an image, which would result in a vector of length $N_T$.

To calculate $\mathbf{S}_b$ we use:

\begin{equation}
\mathbf{S}_b = \mathbf{\mu}_P\mathbf{\mu}^T_P.
\end{equation}

\ac{LDA} aims to minimize $\mathbf{S}_w$ and maximize $\mathbf{S}_b$. To solve this problem we solve the general eigenvalue decomposition or \ac{SVD} of $\mathbf{S}_b\mathbf{S}_w^{-1}$. Which again results in similar calculations for the \ac{PCA} method,

\begin{equation}
\mathbf{S}_b\mathbf{S}_w^{-1} = \mathbf{U}\mathbf{\Sigma}\mathbf{V}^T.
\end{equation}

We now need the eigenvalues for $\mathbf{S}_b\mathbf{S}_w^{-1}$ which are obtained by,

\begin{eqnarray}
\mathbf{S}_b\mathbf{S}_w^{-1}(\mathbf{S}_b\mathbf{S}_w^{-1})^T &=& \mathbf{U}_{LDA}\mathbf{\Sigma}\mathbf{V}^T\mathbf{V}\mathbf{\Sigma}^T\mathbf{U}_{LDA}^T\\
&=& \mathbf{U}_{LDA}\mathbf{\Sigma}\mathbf{\Sigma}^T\mathbf{U}_{LDA}^T\\
&=& \mathbf{U}_{LDA}\mathbf{\Lambda}_{LDA}\mathbf{U}_{LDA}^T.\\
\end{eqnarray}
\noindent
Where once again $\mathbf{U}_{LDA}$ are the eigenvectors with $\mathbf{\Lambda}_{LDA}$ is a diagonal matrix with the eigenvalues. We can perform a dimension reduction based on eigenvalue by keeping the $k$ largest eigenvalues and associated eigenvectors. 

Now that \ac{PCA} and \ac{LDA} are defined we can move forward with the facial recognition algorithm. We first need to project our training images into the eigenspace that our \ac{PCA} or \ac{LDA} produced. 

The term \emph{eigenface} identifies the image in the \ac{PCA} eigenspace. The term \emph{fisherface} is used for an image in the \ac{LDA} space. The term \emph{fisher} is used since $\mathbf{S}_b\mathbf{S}_w^{-1}$ is referred to as the Fisher matrix. From here we define our two \emph{faces} as,

\begin{equation}
\mathbf{F}_E = \mathbf{T}\mathbf{U}_{PCA}~~~~~~~~\mathbf{F}_F = \mathbf{T}\mathbf{U}_{LDA}
\end{equation}	

\cite{eigenfisherface}

% CITATIONClose[8] M. Turk; A. Pentland (1991). "Eigenfaces for recognition" (PDF). Journal of Cognitive Neuroscience. 3 (1): 71–86. doi:10.1162/jocn.1991.3.1.71. PMID 23964806

We are now ready to analyze a new image. Going back to our security system example. We are ready now to start the camera and as people walk up to our door we take an image, after we take the image we vectorize the image, $\mathbf{n}=\operatorname{vec}{\mathbf{P}}$. Next we need to project the new image into the eigenspace of our choosing resulting in $\mathbf{W}$,
\begin{equation}
\mathbf{W}_{PCA} = \mathbf{U}_{PCA}\mathbf{n} ~~~~~~~~~~~~~ \mathbf{W}_{LDA} = \mathbf{U}_{LDA}\mathbf{n}
\end{equation}	

Next we compare $\mathbf{W}$ to the respective $\mathbf{F}$ we subtract and find the magnitude of the result. The calculation results in a vector of distances of $N_T$ length,

\begin{equation}
\mathbf{d} = \norm{\mathbf{F}-\mathbf{W}}^2.
\end{equation}	

The smallest distance represents the most similar image. Some testing is needed depending on the camera resolution and other factors like lighting and the ability to get the person to be looking directly at the camera. In practical systems some experimentation is used to determine ranges for which the distance metric can fall into. If the minimum distance is low enough then the face is recognized. If the minimum distance is low but not low enough to be a match then the face isn't in the library of trained images. Or if the lowest distance is very high the image isn't a face at all. Depending on the system some of the information may not be useful. 
	
\section{Neural Networks}
	<TODO Section Neural Networks : NOT DONE>

\section{Support Vector Machines}
	<TODO Section Support Vector Machines : NOT DONE>

\section{K-Means Clustering}
	<TODO Section K-Means Clustering : NOT DONE>


