\chapter{Overview of Computing Platforms}
	<TODO Chapter Overview of Computing Platforms : PROOF READ>
Today's electronics hobbyist has many platforms available to learn anything from programming basics to complicated topics like web hosting and robotics. This chapter discussed all the possibilities of available types of computing platforms. We discuss Single board computer's, Field Programmable Gate Arrays, and Application Specific Integrated Circuits. All of which have varying levels for skill required to get started. This book will get you started in designing systems for all the platforms.  

If you are a beginner in all this then you should start with single board computers. They are a smaller and cheaper version of your laptop. They offer an operating system and are user friendly. They are not as computationally equipped but do offer some ability to do complex processing. 

A Graphical Processing Unit (GPU) offers more processing power depending on the application. GPUs offer performance gains when the algorithm has a lot of multiply-accumulate operations. However the working memory can be limiting; the overhead of moving data to and from the host CPU degrades performance.

For high-data rate applications where large data sets are ingested by algorithms FPGAs can be used to select significant pieces of the data for analysis. This data selection concept can be a little abstract but for particular applications it can be applied quite effectively. For instance in image processing a portion of the image can be eliminated or just edges are considered. 

Finally, the last step and the ultimate in computational performance is the Application Specific Integrated Circuit. These chips are touched on briefly here for the sake of completeness however the hobbyist would only explore this option if the hobby became an amazing success and would pay for a chip to be manufactured. 
	
	
\section{What Can an FPGA Do}
	<TODO Section What Can an FPGA Do : PROOF READ>

The development of todays FPGAs started in the late 1988s, where Simple Programmable Logic Devices took the place of hand placing logic gates. These SPLDs gave way to Complex Programmable Logic Devices. This section provides a brief history which will help in the understanding of how and why FPGAs are used.  	

\subsection{History of FPGAs}
	<TODO Subsection History of FPGAs : PROOF READ>

Early versions of the \ac{FPGA} were significantly less capable than today's \ac{FPGA}. But even before the \ac{FPGA} there were programmable logic cells that could be configured after \ac{IC} fabrication. These precursors to the \ac{FPGA} were the start of programmable logic being used in data centers and cloud computing, like it is today.

The history of the \ac{FPGA} really includes the \ac{SPLD}, \ac{CPLD}, and the \emph{Macrocell} \ac{IC}s. Each of these \ac{IC}s in turn increase the processing the level of configurable logic.
	
\subsubsection{SPLDs and CPLDs}
	<TODO Subsubsection  SPLDs and CPLDs : PROOF READ>

The Simple Programmable Logic Device \ac{SPLD} has two types, the Programmable Array Logic \ac{PAL} and the Generic Array Logic \ac{GAL}. Both the \ac{PAL} and the \ac{GAL} are Sum of Product \ac{SOP} devices where inputs are sent to a series of \emph{AND} gates then the results of the \emph{AND} gates are \emph{XOR}'d together. The \ac{PAL} \ac{IC} was programmed by burning fuses, and so was programmable only once. The \ac{GAL} used \ac{EEPROM} so the \ac{IC} was reconfigurable.

The Complex Programmable Logic Device \ac{CPLD} consists of array of \ac{SPLD}s with programmable interconnects. The architecture for the \ac{CPLD} allows the user to program the outputs of one \ac{SPLD} to be the inputs to the next \ac{SPLD}. The Programmable Interconnect Array \ac{PIA} allows the interconnection of all the \ac{SPLD}s in a \ac{CPLD}.
	
\subsubsection{Macrocells}
	<TODO Subsubsection  Macrocells : PROOF READ>

A \emph{macrocell} was used in tandem with an \ac{SPLD}. The macrocell added some functionality to the outputs of the \ac{SPLD}. The macrocell consists of an array of \emph{OR} gates. The output of the \emph{OR} gates were then routed to some \emph{output logic}. The vendors of the macrocells would change what functionality of the \emph{output logic} for different families of chips depending on customer demand.
	
\subsection{Why Use an FPGA}
	<TODO Subsection Why Use an FPGA : PROOF READ>
	
FPGAs are really the blank slate of embedded processing. The features in FPGAs today lead to a large range of applications, including Artificial Intelligence and machine learning. FPGAs have many characteristics that make them a great versatile computational platform. 

Most people of familiar with processors, CPUs, or microprocessors. The architecture of these platforms vary great but at the root of each processor has one or more ALU. An FPGA can have an ALU implemented in it. Research in processor design use FPGAs to test new architecture performance and prove out designs. This research is enabled by the ability to reprogram the FPGA to make design changes and test whether the changes are an improvement in practice. 

Another ability that makes FPGAs versatile is the ability to add additional logic in parallel. When an algorithm is being implemented the programmer is looking to optimize the code to get the result as fast as possible. With an FPGA the programmer can add another operation in parallel as opposed to waiting for an Arithmetic Logic Unit to be ready to do the calculation. 

Another benefit of an FPGA is the amount of configurable input and output pins. If your application needs to interface to many peripherals FPGAs have hundreds even thousands of pins available for configuration. Some pins are high performance pins that can be used to interface to DDR, SATA, and PCIe controllers. 

Finally, an FPGA is a great testing platform for ASIC development. The ultimate in performance is the ASIC however you get one chance for a die. The NRE for a second attempt can make it not cost effective.

\subsubsection{Introduction to Data Rate}
	<TODO Subsubsection  Introduction to Data Rate : PROOF READ>

A data rate in general is defined as the number of bits per unit time. For a system the data rate is how much data can the system handle per unit time. The overall data rate of the system is the minimum data rate for the individual components. In designing a system with a minimum data rate requirement it is important to be mindful of the data rates of the components.

To make the concept of determining data rate a little more concrete lets look at a simple architecture of a wireless receiver. To determine the data rate of a system the knowledge of the application and more importantly the architecture of the system needs to be known. Even for a simple receiver there are multiple ways to architect the design to enable higher or lower data rates. 

\subsubsection{Data Rate Study - Image and Video Processing}
	<TODO Subsubsection  Data Rate Study - Image and Video Processing : PROOF READ>

The data rates for image and video processing push the limits of processing power. The algorithms that are used for image and video processing must be able to handle a lot of data and every stage during the processing. Even if the processing is not done in real-time, which is where data is stored in non-volatile memory and is read out as needed for the processing, the algorithms require a certain scope of pixels to be able to make decisions.

When pushing around \ac{HD} data interfacing to high speed memory is a must but also the throughput of internal buses on any embedded processing platform or on an general \ac{CPU} will be tested. There is so much data even just in one \ac{HD} image, for example, a $(1920\times1080)$ pixel image has $2,073,600$ pixels and each pixel in raw image format with $24$ bit color is $6$ \ac{MB}. For a \emph{4K} video image the amount of uncompressed $24$-bit data is $24$ \ac{MB}.

If we start analyzing video we start seeing $30$ high-resolution images a second. For a \emph{1080p} video the data rate is $6(30)=180$ \ac{MB} per second. For the \emph{4K} video the data rate is four times higher at $24*30=720$ \ac{MB} per second. At the \emph{4K} video data rate we are pushing limits of \ac{SSD} read speeds over \ac{SATA}.

The processing of this amount of data must first be to remove all the data that isn't relevant to the application. If the application tracks moving objects when anything that is in the background is not fed through the computationally intensive portions of the algorithms. If pixels aren't changing then why keep them. After significantly reducing the amount of data to process the algorithm is execute faster and will be more likely to be a real-time video processing solution.
	
\section{What Can a SBC Do}
	<TODO Section What Can a SBC Do : PROOF READ>

The \ac{SBC} can do many things. The better question is what can't it do. However, when you get your brand new \ac{SBC} you open the box and plug it in, turn it on and it boots up. You are at the desktop of a Linux distribution that you hadn't heard of before today. You wonder ...now what?

By far the most popular \ac{SBC} is the \ac{RPi} but this discussion really applies to the Aurdino and any other small form factor \ac{SBC} out there. The most exciting projects are the one that you program and they physically interact with the world. This can be anything from automatically watering a plant to driving a robot around your living room.

If your \ac{SBC} is connected to your \ac{WLAN} this is a great opportunity to learn more about networking. You set up your \ac{SBC} to host a \ac{NAS} which shares your files with all your trusted \ac{WLAN} connected devices. You can also learn the basics of web-development. The \ac{SBC} is perfectly capable of hosting a website that can be accessible from anywhere if you also learn how to properly, and securely, configure your router.

The possibilities are many and the internet itself can help you get started in learning about what projects are simple to get you started. And as you learn more about your \ac{SBC} you can post the exciting projects you come up with to help the next person get excited about making with an \ac{SBC}.
	
\subsubsection{The Performance of Decisions}
	<TODO Subsubsection  The Performance of Decisions : PROOF READ>

There are many mechanisms in the modern \ac{CPU} and embedded \ac{uC} to predict is branches or decisions will take the processing in a different direction. Just like when you think you have your day all planned out and something surprises you; you need to derail the rest of your plans.

In the world of embedded processing the act of \emph{derailing your plans} is flushing the fetch pipeline and suffering the read latency for the data that needs to be operated on. The more times this happens the slower the processing takes because the recovery from and the starting up into the new processing stream is time wasted.

The performance of taking a branch to a new section of code in general is not a major issue. When this happens the latency is on the order of \ac{us}. However, if your processing algorithms are at or near the capacity of the device, special care must be taken to squeeze out all the performance you can.
	
\subsubsection{Data Rate Study - Web Hosting}
	<TODO Subsubsection  Data Rate Study - Web Hosting : PROOF READ>

The data rate for a webpage can be a little tricky to compute. The data rate depends first on the connection speed, the user's viewing speed, and the size of the webpage itself. The average webpage size is constantly increasing with faster connections and processing speeds but at the time of writing the average size of a webpage is $3$ \ac{MB} \cite{google}. For this study we will also say we want to page to load, be usable for the requester, in under three seconds.

The information tells us rather quickly that we need about a $1$ \ac{MB} per second connection and processing power to handle today's average webpage. Or maybe we take longer than the $3$ seconds to load the entire page but have the top of the page load quicker. We put a priority on the top of the page and let the requester start looking at the page, maybe we won't even have to load the rest of the page. But getting the data in front of the requester faster produces the illusion that the data rate of rendering the page is large when actually we are still working on getting the data rendered.Â 
	
\section{FPGA vs. Processor}
	<TODO Section FPGA vs. Processor : PROOF READ>

The \ac{FPGA} and the \ac{uC} are two different tools. Each are good at some processing tasks and poor at other programming tasks. The two metrics we will touch on here is data rate capability and decision making. Decision making can also be thought of as \emph{managerial tasks} which are how well the processing platform is able to take inputs either from outside the \ac{IC} itself or process data and make decisions based on the data. In most processing applications it's a combination of both.
	
\subsubsection{Balancing Data Rate and Decision Making}
	<TODO Subsubsection  Balancing Data Rate and Decision Making : PROOF READ>
We will start with data rate capability. The data rate that either processing unit can handle is of course how much data per unit time can the processing unit handle. First the \emph{unit of time} is proportional to the clock frequency. If we just compare the \ac{FPGA} and \ac{uC} the \ac{uC} usually wins with a higher operating frequency. However, the \ac{FPGA} is able to pipeline significantly more data that the \ac{uC} can. Since the \ac{FPGA} hard is reconfigurable we can if we need to make a multiplier, adder, multiplexer, etc at any point in our processing algorithm. With the \ac{uC} we will need to keep the data in memory until the \ac{ALU} is ready to handle the specific operation we need to perform.

The fact that we can make any \ac{ALU} operation we want is great. But even better we can also make as many as we need on an \ac{FPGA}. There is a limited size of the \ac{FPGA}, but for example there are $5000$ multipliers dedicated on some \ac{FPGA}s. We could use all of these in the same clock cycle if our algorithm needed to. So the higher data rate capability goes to the \ac{FPGA}.

As for decision making capability it is easier to use a \ac{uC}. The \ac{FPGA} is perfectly capable of making the same decisions its just that the \ac{uC} is more efficient. Due to the nature of the \ac{FPGA} it is likely that when making a decision between $A$ and $B$ you will fully calculate the results for both, then at the end only output the one that is needed. This way of resource intensive and generally wasteful.

The truly powerful processing unit will have elements of both \ac{FPGA} to handle the high data rate tasks with a \ac{uC} handling decisions and managerial tasks. Which is the reason why we have see the rise in popularity in \ac{SoC}s. Where an \ac{ARM} process is tightly integrated to the \ac{FPGA} fabric. Data can be shared through an shared memory bus.

